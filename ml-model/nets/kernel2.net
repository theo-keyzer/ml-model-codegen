// =========================================================================
// COMPLETE CUDA KERNEL DOMAIN KNOWLEDGE
// =========================================================================

Domain {
  name = cuda_kernels
  version = "1.0.0"
  desc = "CUDA kernel implementations for ML operations"
}

// =========================================================================
// BASIC ARITHMETIC KERNELS
// =========================================================================

Kernel {
  kernel = add_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Element-wise addition kernel"
  signature = "void add_kernel(T* input1, T* input2, T* output, int size);"
  body = """
    template<typename T>
    __global__ void add_kernel(T* input1, T* input2, T* output, int size) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx < size) {
            output[idx] = input1[idx] + input2[idx];
        }
    }
  """
}

KernelOp {
  op = add_dispatch
  parent = add_cuda
  desc = "Element-wise addition dispatch"
  input_rank = 3
  output_rank = 3
  dimension_calc = "int size = input1_shape[0] * input1_shape[1] * input1_shape[2];"
  block_size = 256
  grid_calc = "(size + 255) / 256"
}

// =========================================================================
// MATRIX MULTIPLICATION KERNELS
// =========================================================================

Kernel {
  kernel = matmul_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Matrix multiplication kernel"
  signature = "void matmul_kernel(T* A, T* B, T* C, int M, int N, int K);"
  body = """
    template<typename T>
    __global__ void matmul_kernel(T* A, T* B, T* C, int M, int N, int K) {
        int row = blockIdx.y * blockDim.y + threadIdx.y;
        int col = blockIdx.x * blockDim.x + threadIdx.x;
        
        if (row < M && col < N) {
            T sum = 0;
            for (int i = 0; i < K; i++) {
                sum += A[row * K + i] * B[i * N + col];
            }
            C[row * N + col] = sum;
        }
    }
  """
}

KernelOp {
  op = matmul_dispatch
  parent = matmul_cuda
  desc = "Matrix multiplication operation dispatch"
  input_rank = 2
  output_rank = 2
  dimension_calc = "int M = input_shape[0]; int N = weights_shape[1]; int K = input_shape[1];"
  block_size = 256
  grid_calc = "(M * N + block_size - 1) / block_size"
}

KernelParam {
  param = M
  parent = matmul_cuda
  type = "int"
  calculation = "input_shape[0]"
}

KernelParam {
  param = N
  parent = matmul_cuda
  type = "int"
  calculation = "weights_shape[1]"
}

KernelParam {
  param = K
  parent = matmul_cuda
  type = "int"
  calculation = "input_shape[1]"
}

// =========================================================================
// ACTIVATION FUNCTION KERNELS
// =========================================================================

Kernel {
  kernel = gelu_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "GELU activation kernel"
  signature = "void gelu_kernel(T* input, T* output, int size);"
  body = """
    template<typename T>
    __global__ void gelu_kernel(T* input, T* output, int size) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        if (idx < size) {
            T x = input[idx];
            // GELU approximation: 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))
            T x_cubed = x * x * x;
            T inner = 0.7978845608f * (x + 0.044715f * x_cubed);
            output[idx] = 0.5f * x * (1.0f + tanh(inner));
        }
    }
  """
}

KernelOp {
  op = gelu_dispatch
  parent = gelu_cuda
  desc = "GELU activation dispatch"
  input_rank = 3
  output_rank = 3
  dimension_calc = "int size = input_shape[0] * input_shape[1] * input_shape[2];"
  block_size = 256
  grid_calc = "(size + 255) / 256"
}

// =========================================================================
// NORMALIZATION KERNELS
// =========================================================================

Kernel {
  kernel = layernorm_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Layer normalization kernel"
  signature = "void layernorm_kernel(T* input, T* output, T* gamma, T* beta, int batch, int seq_len, int hidden_dim, float eps);"
  body = """
    template<typename T>
    __global__ void layernorm_kernel(
        T* input, T* output, T* gamma, T* beta, 
        int batch, int seq_len, int hidden_dim, float eps = 1e-5f
    ) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        int total_tokens = batch * seq_len;
        
        if (idx < total_tokens) {
            T* token_input = input + idx * hidden_dim;
            T* token_output = output + idx * hidden_dim;
            
            // Compute mean
            T sum = 0;
            for (int i = 0; i < hidden_dim; i++) {
                sum += token_input[i];
            }
            T mean = sum / hidden_dim;
            
            // Compute variance
            T var_sum = 0;
            for (int i = 0; i < hidden_dim; i++) {
                T diff = token_input[i] - mean;
                var_sum += diff * diff;
            }
            T variance = var_sum / hidden_dim;
            T inv_std = rsqrt(variance + eps);
            
            // Normalize and scale
            for (int i = 0; i < hidden_dim; i++) {
                T normalized = (token_input[i] - mean) * inv_std;
                token_output[i] = gamma[i] * normalized + beta[i];
            }
        }
    }
  """
}

KernelOp {
  op = layernorm_dispatch
  parent = layernorm_cuda
  desc = "Layer normalization dispatch"
  input_rank = 3
  output_rank = 3
  dimension_calc = "int batch = input_shape[0]; int seq_len = input_shape[1]; int hidden_dim = input_shape[2];"
  block_size = 256
  grid_calc = "(batch * seq_len + 255) / 256"
}

Kernel {
  kernel = bn_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Batch normalization kernel"
  signature = "void batch_norm_kernel();"
  body = """
    template<typename T>
    __global__ void batch_norm_kernel(
        T* input, T* output, T* mean, T* variance, 
        T* gamma, T* beta, int N, int C, int H, int W, float eps = 1e-5f
    ) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        int total = N * C * H * W;
        
        if (idx < total) {
            int c = (idx / (H * W)) % C;
            T normalized = (input[idx] - mean[c]) / sqrt(variance[c] + eps);
            output[idx] = gamma[c] * normalized + beta[c];
        }
    }
  """
}

// =========================================================================
// ATTENTION KERNELS
// =========================================================================

Kernel {
  kernel = attention_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Multi-head attention kernel"
  signature = "void attention_kernel(T* query, T* key, T* value, T* output, T* mask, int batch, int num_heads, int seq_len, int head_dim, bool causal);"
  body = """
    template<typename T>
    __global__ void attention_kernel(
        T* query, T* key, T* value, T* output, T* mask,
        int batch, int num_heads, int seq_len, int head_dim, bool causal
    ) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        int total = batch * num_heads * seq_len * seq_len;
        
        if (idx < total) {
            int q_pos = idx % seq_len;
            int head = (idx / seq_len) % num_heads;
            int b = idx / (seq_len * num_heads);
            
            // Compute attention scores
            T scale = rsqrt((T)head_dim);
            
            for (int k_pos = 0; k_pos < seq_len; k_pos++) {
                if (causal && k_pos > q_pos) break;
                
                T score = 0;
                int q_offset = ((b * num_heads + head) * seq_len + q_pos) * head_dim;
                int k_offset = ((b * num_heads + head) * seq_len + k_pos) * head_dim;
                
                for (int d = 0; d < head_dim; d++) {
                    score += query[q_offset + d] * key[k_offset + d];
                }
                score *= scale;
                
                if (mask != NULL) {
                    score += mask[b * seq_len * seq_len + q_pos * seq_len + k_pos];
                }
                
                // Softmax and weighted sum handled in separate pass
            }
        }
    }
  """
}

KernelOp {
  op = attention_dispatch
  parent = attention_cuda
  desc = "Attention operation dispatch"
  input_rank = 4
  output_rank = 3
  dimension_calc = "int batch = query_shape[0]; int num_heads = query_shape[1]; int seq_len = query_shape[2]; int head_dim = query_shape[3];"
  block_size = 256
  grid_calc = "(batch * num_heads * seq_len + 255) / 256"
}

// =========================================================================
// RECURRENT KERNELS
// =========================================================================

Kernel {
  kernel = lstm_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "LSTM cell kernel"
  signature = "void lstm_kernel(T* input, T* hidden, T* cell, T* output, T* weights, int batch, int seq_len, int hidden_dim);"
  body = """
    template<typename T>
    __global__ void lstm_kernel(
        T* input, T* hidden, T* cell, T* output, T* weights,
        int batch, int seq_len, int hidden_dim
    ) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        int total = batch * hidden_dim;
        
        if (idx < total) {
            int b = idx / hidden_dim;
            int h = idx % hidden_dim;
            
            // LSTM gates: input, forget, cell, output
            // This is a simplified version - full implementation would be more complex
            T i_gate = 0, f_gate = 0, c_gate = 0, o_gate = 0;
            
            // Compute gates (simplified)
            for (int t = 0; t < seq_len; t++) {
                // Gate computations would go here
            }
            
            // Update cell and hidden state
            cell[b * hidden_dim + h] = f_gate * cell[b * hidden_dim + h] + i_gate * c_gate;
            hidden[b * hidden_dim + h] = o_gate * tanh(cell[b * hidden_dim + h]);
            output[b * seq_len * hidden_dim + h] = hidden[b * hidden_dim + h];
        }
    }
  """
}

KernelOp {
  op = lstm_dispatch
  parent = lstm_cuda
  desc = "LSTM cell dispatch"
  input_rank = 3
  output_rank = 3
  dimension_calc = "int batch = input_shape[0]; int seq_len = input_shape[1]; int hidden_dim = hidden_state_shape[1];"
  block_size = 256
  grid_calc = "(batch * hidden_dim + 255) / 256"
}

// =========================================================================
// GRAPH NEURAL NETWORK KERNELS
// =========================================================================

Kernel {
  kernel = gat_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Graph Attention Network kernel"
  signature = "void gat_kernel(T* node_features, T* edge_features, T* adjacency, T* output, T* weights, int num_nodes, int num_edges, int feature_dim);"
  body = """
    template<typename T>
    __global__ void gat_kernel(
        T* node_features, T* edge_features, T* adjacency, T* output, T* weights,
        int num_nodes, int num_edges, int feature_dim
    ) {
        int node_id = blockIdx.x * blockDim.x + threadIdx.x;
        
        if (node_id < num_nodes) {
            // Aggregate features from neighbors with attention
            for (int f = 0; f < feature_dim; f++) {
                T aggregated = 0;
                T attention_sum = 0;
                
                // Iterate through neighbors (simplified - assumes dense adjacency)
                for (int neighbor = 0; neighbor < num_nodes; neighbor++) {
                    T edge_weight = adjacency[node_id * num_nodes + neighbor];
                    if (edge_weight > 0) {
                        // Compute attention score
                        T attention = exp(edge_weight); // Simplified
                        attention_sum += attention;
                        aggregated += attention * node_features[neighbor * feature_dim + f];
                    }
                }
                
                // Normalize and write output
                if (attention_sum > 0) {
                    output[node_id * feature_dim + f] = aggregated / attention_sum;
                }
            }
        }
    }
  """
}

KernelOp {
  op = gat_dispatch
  parent = gat_cuda
  desc = "GAT operation dispatch"
  input_rank = 2
  output_rank = 2
  dimension_calc = "int num_nodes = node_features_shape[0]; int feature_dim = node_features_shape[1]; int num_edges = adjacency_num_edges;"
  block_size = 256
  grid_calc = "(num_nodes + 255) / 256"
}

// =========================================================================
// NEURAL ODE KERNELS
// =========================================================================

Kernel {
  kernel = ode_solver_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "ODE solver kernel (Dormand-Prince RK45)"
  signature = "void ode_solver_kernel(T* state_in, T* state_out, T* time_points, T* dynamics_weights, float t_start, float t_end, int state_dim, int num_steps, bool adjoint);"
  body = """
    template<typename T>
    __global__ void ode_solver_kernel(
        T* state_in, T* state_out, T* time_points, T* dynamics_weights,
        float t_start, float t_end, int state_dim, int num_steps, bool adjoint
    ) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        
        if (idx < state_dim) {
            T state = state_in[idx];
            float dt = (t_end - t_start) / num_steps;
            
            // Simplified Euler integration (full DOPRI5 would be more complex)
            for (int step = 0; step < num_steps; step++) {
                float t = t_start + step * dt;
                
                // Compute derivative f(state, t)
                T derivative = 0;
                for (int j = 0; j < state_dim; j++) {
                    derivative += dynamics_weights[idx * state_dim + j] * state_in[j];
                }
                
                // Euler step: state += dt * f(state, t)
                state += dt * derivative;
            }
            
            state_out[idx] = state;
        }
    }
  """
}

KernelOp {
  op = ode_dispatch
  parent = ode_solver_cuda
  desc = "ODE solver dispatch"
  input_rank = 2
  output_rank = 2
  dimension_calc = "int state_dim = state_input_shape[1]; int num_steps = 10;"
  block_size = 256
  grid_calc = "(state_dim + 255) / 256"
}

// =========================================================================
// MIXTURE OF EXPERTS KERNELS
// =========================================================================

Kernel {
  kernel = weighted_sum_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Weighted sum for expert combination"
  signature = "void weighted_sum_kernel(T* expert_outputs, T* gate_weights, T* output, int batch, int seq_len, int hidden_dim, int num_experts);"
  body = """
    template<typename T>
    __global__ void weighted_sum_kernel(
        T* expert_outputs, T* gate_weights, T* output,
        int batch, int seq_len, int hidden_dim, int num_experts
    ) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        int total = batch * seq_len * hidden_dim;
        
        if (idx < total) {
            int token_id = idx / hidden_dim;
            int feat_id = idx % hidden_dim;
            
            T weighted_sum = 0;
            for (int expert = 0; expert < num_experts; expert++) {
                T weight = gate_weights[token_id * num_experts + expert];
                T expert_out = expert_outputs[expert * batch * seq_len * hidden_dim + idx];
                weighted_sum += weight * expert_out;
            }
            
            output[idx] = weighted_sum;
        }
    }
  """
}

KernelOp {
  op = weighted_sum_dispatch
  parent = weighted_sum_cuda
  desc = "Weighted sum dispatch for MoE"
  input_rank = 3
  output_rank = 3
  dimension_calc = "int batch = expert_outputs_shape[0]; int seq_len = expert_outputs_shape[1]; int hidden_dim = expert_outputs_shape[2];"
  block_size = 256
  grid_calc = "(batch * seq_len * hidden_dim + 255) / 256"
}

// =========================================================================
// POOLING KERNELS
// =========================================================================

Kernel {
  kernel = avgpool_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Global average pooling kernel"
  signature = "void avgpool_kernel(T* input, T* output, int N, int C, int H, int W);"
  body = """
    template<typename T>
    __global__ void avgpool_kernel(T* input, T* output, int N, int C, int H, int W) {
        int n = blockIdx.x;
        int c = threadIdx.x;
        
        if (n < N && c < C) {
            T sum = 0;
            for (int h = 0; h < H; h++) {
                for (int w = 0; w < W; w++) {
                    sum += input[((n * C + c) * H + h) * W + w];
                }
            }
            output[n * C + c] = sum / (H * W);
        }
    }
  """
}

// =========================================================================
// FUSED OPERATION KERNELS
// =========================================================================

Kernel {
  kernel = fused_conv_bn_relu_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Fused convolution + batch norm + ReLU kernel"
  signature = "void fused_conv_bn_relu_kernel();"
  body = """
    template<typename T>
    __global__ void fused_conv_bn_relu_kernel(
        T* input, T* output, T* weights, T* mean, T* variance,
        T* gamma, T* beta, int N, int C_in, int C_out, int H, int W,
        int KH, int KW, int stride, float eps = 1e-5f
    ) {
        int idx = blockIdx.x * blockDim.x + threadIdx.x;
        int H_out = (H - KH) / stride + 1;
        int W_out = (W - KW) / stride + 1;
        int total = N * C_out * H_out * W_out;
        
        if (idx < total) {
            int w_out = idx % W_out;
            int h_out = (idx / W_out) % H_out;
            int c_out = (idx / (W_out * H_out)) % C_out;
            int n = idx / (W_out * H_out * C_out);
            
            // Convolution
            T sum = 0;
            for (int c_in = 0; c_in < C_in; c_in++) {
                for (int kh = 0; kh < KH; kh++) {
                    for (int kw = 0; kw < KW; kw++) {
                        int h_in = h_out * stride + kh;
                        int w_in = w_out * stride + kw;
                        sum += input[((n * C_in + c_in) * H + h_in) * W + w_in] *
                               weights[((c_out * C_in + c_in) * KH + kh) * KW + kw];
                    }
                }
            }
            
            // Batch norm
            T normalized = (sum - mean[c_out]) / sqrt(variance[c_out] + eps);
            T bn_output = gamma[c_out] * normalized + beta[c_out];
            
            // ReLU
            output[idx] = bn_output > 0 ? bn_output : 0;
        }
    }
  """
}

Kernel {
  kernel = threshold_eval_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Evaluate threshold kernel"
  signature = "bool evaluate_threshold(float* tensor, float threshold);"
  body = """
    __device__ bool evaluate_threshold(float* tensor, float threshold) {
        // Simple threshold evaluation - can be made more sophisticated
        return tensor[0] > threshold;
    }
  """
}

Kernel {
  kernel = expert_routing_cuda
  parent = cuda_kernels
  backend = cuda
  desc = "Expert routing kernel"
  signature = "void expert_routing_kernel(float* gate_logits, int num_experts, int top_k);"
  body = """
    __global__ void expert_routing_kernel(float* gate_logits, int num_experts, int top_k) {
        int token_id = blockIdx.x * blockDim.x + threadIdx.x;
        // Top-K selection logic here
        // ... implementation ...
    }
  """
}

// =========================================================================
// ENUMERATION RULES
// =========================================================================

TargetRule {
  target = gpu_a100
  arch = ampere
  vendor = nvidia
}

TargetRule {
  target = gpu_v100
  arch = volta
  vendor = nvidia
}

TargetRule {
  target = gpu_mobile
  arch = mali
  vendor = arm
}

TargetRule {
  target = gpu_amd
  arch = rdna2
  vendor = amd
}

DtypeRule {
  dtype = fp32
  bits = 32
  type = float
}

DtypeRule {
  dtype = fp16
  bits = 16
  type = float
}

DtypeRule {
  dtype = bf16
  bits = 16
  type = float
}

DtypeRule {
  dtype = int8
  bits = 8
  type = int
}

DtypeRule {
  dtype = int32
  bits = 32
  type = int
}

FlagRule {
  flag = fused_ops
  desc = "Enable operation fusion optimizations"
}

FlagRule {
  flag = fp16_mixed
  desc = "Use mixed precision FP16/FP32"
}

FlagRule {
  flag = int8_quantize
  desc = "Quantize to INT8"
}

FlagRule {
  flag = memory_optimize
  desc = "Optimize for memory usage"
}

FlagRule {
  flag = sparse_attention
  desc = "Use sparse attention patterns"
}

FlagRule {
  flag = kernel_fusion
  desc = "Fuse compatible kernels"
}

FlagRule {
  flag = async_compute
  desc = "Enable asynchronous computation"
}
